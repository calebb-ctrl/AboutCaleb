To me, ethical ambiguity in modeling is the grey area where technical/code success and social harm intertwine. For example it shows up when we use proxy variables like "zip code" as a stand in for creditworthiness that inadvertently intergrates systemic inequality into an algorithm.

Here are some ideas to keep in mind when dealing with ethical ambiguity:

Stakeholder Engagement: We cannot and shouldn't model communities and things we don’t understand. By involving those impacted by the model during the design phase, we go from just observing the data to understanding the people behind the numbers and their experiences. This helps identify potential harm that a data scientist might miss.

Model Interpretability: When creating a model if we cannot explain why the model reached a specific conclusion, we cannot effectively check it for bias. Interpretability allows us to ensure the model is making decisions based on relevant logic rather than hidden prejudices that were inadvertaly put into the program.

Ongoing Monitoring: Data isn't static, it is always changing and growing. A model that is ethical at launch can end up being unethical as social conditions change. Continuous monitoring of the model ensures that as the real world evolves, our models don't become tools for unintentional discrimination.

For example we could consider a model used in Charlotte to predict housing instability. Theoretically it might accurately identify at risk neighborhoods but if that data is then used by predatory lenders rather than social services, then the model’s success becomes an ethical issue.

As data scientists our goal shouldn't just be to build models that work, but to build models that assist and better society. By enacting these practices we can begin to ensure that our models servie people ethically and good.
